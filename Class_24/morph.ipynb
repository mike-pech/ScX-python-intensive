{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'way',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'is',\n",
       " 'to',\n",
       " 'quit',\n",
       " 'talking',\n",
       " 'and',\n",
       " 'begin',\n",
       " 'doing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'The way to get started is to quit talking and begin doing'\n",
    "regex = nltk.tokenize.RegexpTokenizer(pattern=\"\\w+\")\n",
    "tokenized = regex.tokenize(text)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация — сродни аккуратному хирургическому воздействию на слово с восстановлением его к начальной форме"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'way',\n",
       " 'to',\n",
       " 'get',\n",
       " 'started',\n",
       " 'is',\n",
       " 'to',\n",
       " 'quit',\n",
       " 'talking',\n",
       " 'and',\n",
       " 'begin',\n",
       " 'doing']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm = WordNetLemmatizer()\n",
    "lemmatized = [lemm.lemmatize(word) for word in tokenized]\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Через pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq pymorphy2 pymorphy2-dicts-ru pymorphy2-dicts-uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer(lang=\"ru\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стали\n",
      "VERB,perf,intr plur,past,indc\n",
      "стать\n",
      "0.975342\n",
      "((DictionaryAnalyzer(), 'стали', 945, 4),)\n",
      "\n",
      "стали\n",
      "NOUN,inan,femn sing,gent\n",
      "сталь\n",
      "0.010958\n",
      "((DictionaryAnalyzer(), 'стали', 13, 1),)\n",
      "\n",
      "стали\n",
      "NOUN,inan,femn plur,nomn\n",
      "сталь\n",
      "0.005479\n",
      "((DictionaryAnalyzer(), 'стали', 13, 6),)\n",
      "\n",
      "стали\n",
      "NOUN,inan,femn sing,datv\n",
      "сталь\n",
      "0.002739\n",
      "((DictionaryAnalyzer(), 'стали', 13, 2),)\n",
      "\n",
      "стали\n",
      "NOUN,inan,femn sing,loct\n",
      "сталь\n",
      "0.002739\n",
      "((DictionaryAnalyzer(), 'стали', 13, 5),)\n",
      "\n",
      "стали\n",
      "NOUN,inan,femn plur,accs\n",
      "сталь\n",
      "0.002739\n",
      "((DictionaryAnalyzer(), 'стали', 13, 9),)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in morph.parse(\"стали\"):\n",
    "    for i in entry:\n",
    "        print(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='капибара', tag=OpencorporaTag('NOUN,anim,femn sing,nomn'), normal_form='капибара', score=1.0, methods_stack=((DictionaryAnalyzer(), 'капибара', 53, 0),))]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse(\"Капибара\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг — сродни взять пилу и отхуячить все \"лишние\" части слова кроме основы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'get',\n",
       " 'start',\n",
       " 'is',\n",
       " 'to',\n",
       " 'quit',\n",
       " 'talk',\n",
       " 'and',\n",
       " 'begin',\n",
       " 'do']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "stemmed = [stemmer.stem(word) for word in tokenized]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['у', 'лукомор', 'дуб', 'зелен', 'злат', 'цеп', 'на', 'дуб', 'том']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(language=\"russian\")\n",
    "text = 'У Лукоморья дуб зеленый, златая цепь на дубе том'\n",
    "tokenized = nltk.tokenize.RegexpTokenizer(\"\\w+\").tokenize(text)\n",
    "stemmed = [stemmer.stem(word) for word in tokenized]\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
