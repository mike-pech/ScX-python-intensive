# Оптимизаторы
```Если коротко, используй Adam, лол.``` </p>
__Оптимизаторы__ — это алгоритмы или функции, которые адаптируют гиперпараметры (learning rate) и веса нейронки для лучшего обучения.<br>
__Моментум__ (сука, слово номальное есть ИНЕРЦИЯ) — помогает сделать движение по градиентному спуску сильнее. Таким образом, функция движется к наименьшей точке быстрее.</p>
__RMSProp__ — МОЩНЫЙ оптимизатор, но Адам лучше, лол. Отлично подойдёт для больших датасетов, где очень разнообразные данные, позволяя долгое время не переобучаться. </p>
